{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/DLI_Header.png\" style=\"width: 400px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations on going through today's course! Hopefully, you've learned some valuable skills along the way and had fun doing it. Now it's time to put those skills to the test. In this assessment, you will train a new model that is able to recognize fresh and rotten fruit. You will need to get the model to a validation accuracy of `92%` in order to pass the assessment, though we challenge you to do even better if you can. You will have the use the skills that you learned in the previous exercises. Specifically, we suggest using some combination of transfer learning, data augmentation, and fine tuning. Once you have trained the model to be at least 92% accurate on the validation dataset, save your model, and then assess its accuracy. Let's get started! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, you will train a model to recognize fresh and rotten fruits. The dataset comes from [Kaggle](https://www.kaggle.com/sriramr/fruits-fresh-and-rotten-for-classification), a great place to go if you're interested in starting a project after this class. The dataset structure is in the `data/fruits` folder. There are 6 categories of fruits: fresh apples, fresh oranges, fresh bananas, rotten apples, rotten oranges, and rotten bananas. This will mean that your model will require an output layer of 6 neurons to do the categorization successfully. You'll also need to compile the model with `categorical_crossentropy`, as we have more than two categories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/fruits.png\" style=\"width: 600px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load ImageNet Base Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We encourage you to start with a model pretrained on ImageNet. Load the model with the correct weights, set an input shape, and choose to remove the last layers of the model. Remember that images have three dimensions: a height, and width, and a number of channels. Because these pictures are in color, there will be three channels for red, green, and blue. We've filled in the input shape for you. This cannot be changed or the assessment will fail. If you need a reference for setting up the pretrained model, please take a look at [notebook 05b](05b_presidential_doggy_door.ipynb) where we implemented transfer learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-26 01:36:23.092815: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-01-26 01:36:23.188070: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-01-26 01:36:23.188091: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-01-26 01:36:23.213438: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-01-26 01:36:23.249390: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-26 01:36:26.190260: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-01-26 01:36:26.338455: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-01-26 01:36:26.338517: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-01-26 01:36:26.340388: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-01-26 01:36:26.340440: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-01-26 01:36:26.340455: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-01-26 01:36:26.513160: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-01-26 01:36:26.513237: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-01-26 01:36:26.513244: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2022] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2024-01-26 01:36:26.513271: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-01-26 01:36:26.513513: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 7537 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3080, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "base_model = keras.applications.VGG16(\n",
    "    weights='imagenet',\n",
    "    input_shape=(224, 224, 3),\n",
    "    include_top=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Freeze Base Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we suggest freezing the base model, as done in [notebook 05b](05b_presidential_doggy_door.ipynb). This is done so that all the learning from the ImageNet dataset does not get destroyed in the initial training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze base model\n",
    "base_model.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Layers to Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to add layers to the pretrained model. [Notebook 05b](05b_presidential_doggy_door.ipynb) can be used as a guide. Pay close attention to the last dense layer and make sure it has the correct number of neurons to classify the different types of fruit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create inputs with correct shape\n",
    "inputs = keras.Input(shape=(224, 224, 3))\n",
    "\n",
    "x = base_model(inputs, training=False)\n",
    "\n",
    "# Add pooling layer or flatten layer\n",
    "x = keras.layers.GlobalAveragePooling2D()(x)\n",
    "\n",
    "# Add final dense layer\n",
    "outputs = keras.layers.Dense(6, activation = 'softmax')(x)\n",
    "\n",
    "# Combine inputs and outputs to create model\n",
    "model = keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " vgg16 (Functional)          (None, 7, 7, 512)         14714688  \n",
      "                                                                 \n",
      " global_average_pooling2d (  (None, 512)               0         \n",
      " GlobalAveragePooling2D)                                         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 6)                 3078      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14717766 (56.14 MB)\n",
      "Trainable params: 3078 (12.02 KB)\n",
      "Non-trainable params: 14714688 (56.13 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to compile the model with loss and metrics options. Remember that we're training on a number of different categories, rather than a binary classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy' , metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augment the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you'd like, try to augment the data to improve the dataset. Feel free to look at [notebook 04a](04a_asl_augmentation.ipynb) and [notebook 05b](05b_presidential_doggy_door.ipynb) for augmentation examples. There is also documentation for the [Keras ImageDataGenerator class](https://keras.io/api/preprocessing/image/#imagedatagenerator-class). This step is optional, but it may be helpful to get to 92% accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "datagen = ImageDataGenerator(samplewise_center=True,\n",
    "                            rotation_range=10,\n",
    "                            zoom_range = 0.1,\n",
    "                            width_shift_range=0.1, \n",
    "                            height_shift_range=0.1,  \n",
    "                            horizontal_flip=True, \n",
    "                            vertical_flip=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to load the train and validation datasets. Pick the right folders, as well as the right `target_size` of the images (it needs to match the height and width input of the model you've created). If you'd like a reference, you can check out [notebook 05b](05b_presidential_doggy_door.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1182 images belonging to 6 classes.\n",
      "Found 329 images belonging to 6 classes.\n"
     ]
    }
   ],
   "source": [
    "# load and iterate training dataset\n",
    "train_it = datagen.flow_from_directory('data/fruits/train/', \n",
    "                                       target_size=(224,224), \n",
    "                                       color_mode='rgb', \n",
    "                                       class_mode=\"categorical\")\n",
    "# load and iterate validation dataset\n",
    "valid_it = datagen.flow_from_directory('data/fruits/valid/', \n",
    "                                      target_size=(224,224), \n",
    "                                      color_mode='rgb', \n",
    "                                      class_mode=\"categorical\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to train the model! Pass the `train` and `valid` iterators into the `fit` function, as well as setting your desired number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-26 01:45:12.335928: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8800\n",
      "2024-01-26 01:45:14.470877: I external/local_xla/xla/service/service.cc:168] XLA service 0x7f0d752c9800 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-01-26 01:45:14.470912: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 3080, Compute Capability 8.6\n",
      "2024-01-26 01:45:14.503730: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3/36 [=>............................] - ETA: 2s - loss: 5.9298 - accuracy: 0.1771"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1706251514.678402 1812512 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36/36 [==============================] - 21s 498ms/step - loss: 2.3561 - accuracy: 0.5000 - val_loss: 1.1692 - val_accuracy: 0.6778\n",
      "Epoch 2/30\n",
      "36/36 [==============================] - 16s 436ms/step - loss: 0.6895 - accuracy: 0.7733 - val_loss: 0.6003 - val_accuracy: 0.8176\n",
      "Epoch 3/30\n",
      "36/36 [==============================] - 17s 456ms/step - loss: 0.3424 - accuracy: 0.8849 - val_loss: 0.4055 - val_accuracy: 0.8784\n",
      "Epoch 4/30\n",
      "36/36 [==============================] - 17s 450ms/step - loss: 0.2307 - accuracy: 0.9171 - val_loss: 0.3091 - val_accuracy: 0.9210\n",
      "Epoch 5/30\n",
      "36/36 [==============================] - 17s 461ms/step - loss: 0.1618 - accuracy: 0.9416 - val_loss: 0.2496 - val_accuracy: 0.9149\n",
      "Epoch 6/30\n",
      "36/36 [==============================] - 31s 847ms/step - loss: 0.1192 - accuracy: 0.9594 - val_loss: 0.3562 - val_accuracy: 0.9027\n",
      "Epoch 7/30\n",
      "36/36 [==============================] - 17s 463ms/step - loss: 0.1085 - accuracy: 0.9611 - val_loss: 0.1742 - val_accuracy: 0.9514\n",
      "Epoch 8/30\n",
      "36/36 [==============================] - 17s 455ms/step - loss: 0.0753 - accuracy: 0.9695 - val_loss: 0.1876 - val_accuracy: 0.9453\n",
      "Epoch 9/30\n",
      "36/36 [==============================] - 17s 463ms/step - loss: 0.0671 - accuracy: 0.9712 - val_loss: 0.1798 - val_accuracy: 0.9422\n",
      "Epoch 10/30\n",
      "36/36 [==============================] - 17s 452ms/step - loss: 0.0647 - accuracy: 0.9780 - val_loss: 0.1604 - val_accuracy: 0.9666\n",
      "Epoch 11/30\n",
      "36/36 [==============================] - 17s 450ms/step - loss: 0.0450 - accuracy: 0.9865 - val_loss: 0.1379 - val_accuracy: 0.9544\n",
      "Epoch 12/30\n",
      "36/36 [==============================] - 17s 470ms/step - loss: 0.0436 - accuracy: 0.9873 - val_loss: 0.1419 - val_accuracy: 0.9696\n",
      "Epoch 13/30\n",
      "36/36 [==============================] - 16s 435ms/step - loss: 0.0388 - accuracy: 0.9907 - val_loss: 0.1627 - val_accuracy: 0.9544\n",
      "Epoch 14/30\n",
      "36/36 [==============================] - 16s 434ms/step - loss: 0.0340 - accuracy: 0.9898 - val_loss: 0.1534 - val_accuracy: 0.9605\n",
      "Epoch 15/30\n",
      "36/36 [==============================] - 16s 427ms/step - loss: 0.0313 - accuracy: 0.9907 - val_loss: 0.1157 - val_accuracy: 0.9757\n",
      "Epoch 16/30\n",
      "36/36 [==============================] - 16s 430ms/step - loss: 0.0242 - accuracy: 0.9907 - val_loss: 0.1508 - val_accuracy: 0.9514\n",
      "Epoch 17/30\n",
      "36/36 [==============================] - 16s 438ms/step - loss: 0.0247 - accuracy: 0.9915 - val_loss: 0.1504 - val_accuracy: 0.9574\n",
      "Epoch 18/30\n",
      "36/36 [==============================] - 17s 454ms/step - loss: 0.0141 - accuracy: 0.9975 - val_loss: 0.1208 - val_accuracy: 0.9605\n",
      "Epoch 19/30\n",
      "36/36 [==============================] - 16s 445ms/step - loss: 0.0360 - accuracy: 0.9856 - val_loss: 0.1086 - val_accuracy: 0.9666\n",
      "Epoch 20/30\n",
      "36/36 [==============================] - 16s 437ms/step - loss: 0.0161 - accuracy: 0.9966 - val_loss: 0.1554 - val_accuracy: 0.9666\n",
      "Epoch 21/30\n",
      "36/36 [==============================] - 16s 433ms/step - loss: 0.0192 - accuracy: 0.9941 - val_loss: 0.1022 - val_accuracy: 0.9696\n",
      "Epoch 22/30\n",
      "36/36 [==============================] - 16s 432ms/step - loss: 0.0139 - accuracy: 0.9966 - val_loss: 0.1275 - val_accuracy: 0.9666\n",
      "Epoch 23/30\n",
      "36/36 [==============================] - 16s 432ms/step - loss: 0.0198 - accuracy: 0.9958 - val_loss: 0.1105 - val_accuracy: 0.9696\n",
      "Epoch 24/30\n",
      "36/36 [==============================] - 16s 444ms/step - loss: 0.0161 - accuracy: 0.9924 - val_loss: 0.1136 - val_accuracy: 0.9757\n",
      "Epoch 25/30\n",
      "36/36 [==============================] - 16s 446ms/step - loss: 0.0086 - accuracy: 0.9975 - val_loss: 0.1527 - val_accuracy: 0.9666\n",
      "Epoch 26/30\n",
      "36/36 [==============================] - 17s 457ms/step - loss: 0.0145 - accuracy: 0.9966 - val_loss: 0.1312 - val_accuracy: 0.9757\n",
      "Epoch 27/30\n",
      "36/36 [==============================] - 17s 455ms/step - loss: 0.0105 - accuracy: 0.9975 - val_loss: 0.1280 - val_accuracy: 0.9605\n",
      "Epoch 28/30\n",
      "36/36 [==============================] - 17s 452ms/step - loss: 0.0091 - accuracy: 0.9983 - val_loss: 0.1669 - val_accuracy: 0.9574\n",
      "Epoch 29/30\n",
      "36/36 [==============================] - 17s 457ms/step - loss: 0.0105 - accuracy: 0.9966 - val_loss: 0.1319 - val_accuracy: 0.9757\n",
      "Epoch 30/30\n",
      "36/36 [==============================] - 17s 454ms/step - loss: 0.0072 - accuracy: 0.9992 - val_loss: 0.1238 - val_accuracy: 0.9787\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f0ed807a1d0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_it,\n",
    "          validation_data=valid_it,\n",
    "          steps_per_epoch=train_it.samples/train_it.batch_size,\n",
    "          validation_steps=valid_it.samples/valid_it.batch_size,\n",
    "          epochs=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unfreeze Model for Fine Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have reached 92% validation accuracy already, this next step is optional. If not, we suggest fine tuning the model with a very low learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unfreeze the base model\n",
    "base_model.trainable = True\n",
    "\n",
    "# Compile the model with a low learning rate\n",
    "model.compile(optimizer=keras.optimizers.RMSprop(learning_rate = 0.00001),\n",
    "              loss ='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/16\n",
      "36/36 [==============================] - 41s 713ms/step - loss: 0.0395 - accuracy: 0.9898 - val_loss: 0.1241 - val_accuracy: 0.9696\n",
      "Epoch 2/16\n",
      "36/36 [==============================] - 16s 437ms/step - loss: 0.0342 - accuracy: 0.9898 - val_loss: 0.1452 - val_accuracy: 0.9635\n",
      "Epoch 3/16\n",
      "36/36 [==============================] - 17s 449ms/step - loss: 0.0093 - accuracy: 0.9966 - val_loss: 0.0888 - val_accuracy: 0.9848\n",
      "Epoch 4/16\n",
      "36/36 [==============================] - 16s 433ms/step - loss: 0.0182 - accuracy: 0.9966 - val_loss: 0.1721 - val_accuracy: 0.9666\n",
      "Epoch 5/16\n",
      "36/36 [==============================] - 16s 433ms/step - loss: 0.0020 - accuracy: 0.9992 - val_loss: 0.3525 - val_accuracy: 0.9514\n",
      "Epoch 6/16\n",
      "36/36 [==============================] - 16s 430ms/step - loss: 0.0126 - accuracy: 0.9966 - val_loss: 0.1815 - val_accuracy: 0.9726\n",
      "Epoch 7/16\n",
      "36/36 [==============================] - 16s 427ms/step - loss: 5.6077e-04 - accuracy: 1.0000 - val_loss: 0.1431 - val_accuracy: 0.9726\n",
      "Epoch 8/16\n",
      "36/36 [==============================] - 16s 427ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 0.2423 - val_accuracy: 0.9605\n",
      "Epoch 9/16\n",
      "36/36 [==============================] - 16s 433ms/step - loss: 0.0350 - accuracy: 0.9907 - val_loss: 0.1843 - val_accuracy: 0.9726\n",
      "Epoch 10/16\n",
      "36/36 [==============================] - 16s 435ms/step - loss: 2.4280e-04 - accuracy: 1.0000 - val_loss: 0.1746 - val_accuracy: 0.9787\n",
      "Epoch 11/16\n",
      "36/36 [==============================] - 17s 446ms/step - loss: 0.0106 - accuracy: 0.9958 - val_loss: 0.1426 - val_accuracy: 0.9605\n",
      "Epoch 12/16\n",
      "36/36 [==============================] - 17s 448ms/step - loss: 0.0065 - accuracy: 0.9983 - val_loss: 0.1496 - val_accuracy: 0.9787\n",
      "Epoch 13/16\n",
      "36/36 [==============================] - 17s 449ms/step - loss: 0.0083 - accuracy: 0.9949 - val_loss: 0.1263 - val_accuracy: 0.9787\n",
      "Epoch 14/16\n",
      "36/36 [==============================] - 17s 450ms/step - loss: 1.1320e-04 - accuracy: 1.0000 - val_loss: 0.0959 - val_accuracy: 0.9848\n",
      "Epoch 15/16\n",
      "36/36 [==============================] - 16s 439ms/step - loss: 1.1268e-04 - accuracy: 1.0000 - val_loss: 0.1349 - val_accuracy: 0.9848\n",
      "Epoch 16/16\n",
      "36/36 [==============================] - 16s 437ms/step - loss: 1.5310e-04 - accuracy: 1.0000 - val_loss: 0.1910 - val_accuracy: 0.9726\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f0ed80b3a00>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_it,\n",
    "          validation_data=valid_it,\n",
    "          steps_per_epoch=train_it.samples/train_it.batch_size,\n",
    "          validation_steps=valid_it.samples/valid_it.batch_size,\n",
    "          epochs=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully, you now have a model that has a validation accuracy of 92% or higher. If not, you may want to go back and either run more epochs of training, or adjust your data augmentation. \n",
    "\n",
    "Once you are satisfied with the validation accuracy, evaluate the model by executing the following cell. The evaluate function will return a tuple, where the first value is your loss, and the second value is your accuracy. To pass, the model will need have an accuracy value of `92% or higher`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 4s 355ms/step - loss: 0.1267 - accuracy: 0.9818\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.12666772305965424, 0.9817629456520081]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(valid_it, steps=valid_it.samples/valid_it.batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the Assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To assess your model run the following two cells.\n",
    "\n",
    "**NOTE:** `run_assessment` assumes your model is named `model` and your validation data iterator is called `valid_it`. If for any reason you have modified these variable names, please update the names of the arguments passed to `run_assessment`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from run_assessment import run_assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_assessment(model, valid_it)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate a Certificate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you passed the assessment, please return to the course page (shown below) and click the \"ASSESS TASK\" button, which will generate your certificate for the course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/assess_task.png\" style=\"width: 800px;\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
